# Titanic Survival Prediction: Feature Engineering Showcase

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Kaggle](https://img.shields.io/badge/Kaggle-74.64%25-blue.svg)
![XGBoost](https://img.shields.io/badge/XGBoost-Gradient%20Boosting-brightgreen.svg)
![Status](https://img.shields.io/badge/Status-Complete-success.svg)

## ğŸ¯ Project Overview

A feature engineering showcase using the classic Titanic dataset to demonstrate data preprocessing, creative feature creation, and model comparison. Through **title extraction, family grouping, and careful imputation**, this project achieves **74.64% accuracy** on Kaggle's test set.

**Key Achievement**: Demonstrates that thoughtful feature engineering can outperform raw data approaches, with creative features like title extraction and family size improving model performance.

---

## ğŸ“Š Results

### **Kaggle Submission**
- **Test Accuracy**: **74.64%**
- **Rank**: Competitive performance on classic benchmark

### **Model Comparison (5-Fold Cross-Validation)**

| Model | CV Accuracy | Std Dev | Best For |
|-------|-------------|---------|----------|
| **LightGBM** | **83.62%** | 0.0327 | Best overall |
| XGBoost | 83.61% | 0.0217 | Tied for best |
| Random Forest | 83.39% | 0.0357 | Ensemble baseline |
| Logistic Regression | 79.46% | 0.0160 | Linear baseline |

**Winner**: LightGBM and XGBoost are **statistically tied** (difference < 0.01%)

---

## ğŸ”§ Feature Engineering Pipeline

### **1. Missing Value Imputation**

| Feature | Missing % | Strategy | Justification |
|---------|-----------|----------|---------------|
| **Age** | 19.87% | Mean (29.70) | Central tendency preserves distribution |
| **Cabin** | 77.10% | â†’ **HasCabin** (binary) | Presence indicates wealth/priority |
| **Embarked** | 0.22% | Mode ('S') | Most common port |
| **Fare** | 0.24% (test) | Median (14.45) | Robust to outliers |

**Key Decision**: Instead of imputing 77% missing cabin data, created **HasCabin** feature (0/1).

---

### **2. Title Extraction (Regex Feature Engineering)**

```python
def extract_title(name):
    """Extract title from name: 'Braund, Mr. Owen Harris' â†’ 'Mr'"""
    title_search = re.search(' ([A-Za-z]+)\.', name)
    return title_search.group(1) if title_search else ""
```

**Raw Titles Found**: Mr, Mrs, Miss, Master, Dr, Rev, Col, Major, Capt, Countess, etc.

**Grouped into 6 Categories**:
```python
- Mr        â†’ 'Mr' (517 samples)
- Miss/Mlle/Ms â†’ 'Miss' (185 samples)
- Mrs/Mme   â†’ 'Mrs' (126 samples)
- Master    â†’ 'Master' (40 samples - young boys)
- Dr/Rev/Col/Major/Capt â†’ 'Officer' (18 samples)
- Lady/Sir/Countess/Don/Dona â†’ 'Royalty' (5 samples)
```

**Survival Rates by Title**:
```
Mrs      â†’ 79.4% (married women prioritized)
Miss     â†’ 70.3% (unmarried women/girls)
Royalty  â†’ 60.0% (nobility)
Master   â†’ 57.5% (young boys)
Officer  â†’ 27.8% (stayed to help)
Mr       â†’ 15.7% (adult men)
```

**Impact**: Title captures both **gender** AND **social status** in single feature.

---

### **3. Family Features**

```python
# FamilySize = siblings/spouses + parents/children + self
FamilySize = SibSp + Parch + 1

# IsAlone = traveling solo (1) or with family (0)
IsAlone = (FamilySize == 1).astype(int)
```

**Survival by Family Size**:
```
Size 4    â†’ 72.4% survival (optimal family size!)
Size 3    â†’ 57.8%
Size 2    â†’ 55.3%
Size 1    â†’ 30.4% (alone = vulnerable)
Size 8+   â†’ 0.0% (large families overwhelmed)
```

**Insight**: "Women and children first" worked best for **medium-sized families** (3-4 people).

---

### **4. Feature Encoding**

**Numerical Encoding** (used in final model):
```python
Sex:      male=1, female=0
Embarked: S=0, C=1, Q=2
Title:    Label encoded (0-5)
```

**Why not one-hot?**
- Fewer features â†’ simpler model
- Tree-based models (XGBoost, LightGBM) handle ordinal well
- Reduced overfitting risk

---

## ğŸ“ˆ Data Analysis Insights

### **Survival Patterns**

| Factor | Survival Rate | Insight |
|--------|--------------|---------|
| **Female** | 74.2% | "Women and children first" policy |
| **Male** | 18.9% | Men prioritized women/children |
| **1st Class** | 62.9% | Wealth = access to lifeboats |
| **2nd Class** | 47.3% | Middle ground |
| **3rd Class** | 24.2% | Limited access, lower decks |
| **With Family** | 50.6% | Better than alone (30.4%) |
| **Traveling Alone** | 30.4% | Vulnerable |

**Chivalry & Wealth**: Two dominant factors determining survival.

---

## ğŸ› ï¸ Technical Implementation

### **1. Data Preprocessing**

```python
# Load data
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')

# Drop PassengerId (not predictive)
df_train = df_train.drop('PassengerId', axis=1)

# Impute missing values (see table above)
df_train['Age'].fillna(df_train['Age'].mean(), inplace=True)
df_train['HasCabin'] = df_train['Cabin'].notna().astype(int)

# Extract titles
df_train['Title'] = df_train['Name'].apply(extract_title)
df_train['Title'] = df_train['Title'].apply(simplify_title)

# Engineer family features
df_train['FamilySize'] = df_train['SibSp'] + df_train['Parch'] + 1
df_train['IsAlone'] = (df_train['FamilySize'] == 1).astype(int)

# Drop non-predictive features
df_train = df_train.drop(['Name', 'Ticket', 'Cabin'], axis=1)
```

---

### **2. Model Training**

```python
from sklearn.model_selection import train_test_split, cross_val_score
from lightgbm import LGBMClassifier

# Split data
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Train LightGBM
model = LGBMClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    scale_pos_weight=1.61,  # Handle class imbalance (61.6% died, 38.4% survived)
    random_state=42
)
model.fit(X_train, y_train)

# Evaluate
accuracy = model.score(X_val, y_val)
cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
print(f"Validation Accuracy: {accuracy:.4f}")
print(f"CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
```

---

### **3. Feature Importance (LightGBM)**

| Feature | Importance | Interpretation |
|---------|-----------|----------------|
| **Fare** | 512 | Ticket price = wealth/class |
| **Age** | 357 | Children prioritized |
| **FamilySize** | 75 | Optimal size = better survival |
| **Title** | 58 | Gender + social status |
| **Pclass** | 48 | 1st class advantages |
| **Sex** | 47 | Female survival rate 4x male |

**Insight**: **Fare** is most important (captures wealth), followed by **Age** (children first).

---

## ğŸ“ What I Learned

### **Feature Engineering Lessons**:

1. **Title extraction matters** - Captures nuances that raw gender misses
2. **Binary features can be powerful** - HasCabin (0/1) > imputing 77% missing data
3. **Domain knowledge helps** - Understanding "women and children first" guided FamilySize feature
4. **Less is more** - Numerical encoding outperformed one-hot (less overfitting)

### **Modeling Insights**:

1. **Gradient boosting dominates** - XGBoost/LightGBM > RandomForest > LogisticRegression
2. **Class weighting helps** - scale_pos_weight=1.61 for 61/39 imbalance
3. **Stratified splits essential** - Maintain 61/39 ratio in train/val/test
4. **Tree-based models don't need scaling** - Saved preprocessing step

### **Data Science Process**:

1. **EDA first** - Understand patterns before feature engineering
2. **Iterative improvement** - Baseline (79%) â†’ Feature engineering (83%) â†’ Tuning (84%)
3. **Validate assumptions** - Check survival rates for each feature
4. **Simple can win** - Numerical encoding beat complex one-hot

---

## ğŸ“ Repository Structure

```
titanic-survival-prediction/
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ Bruce_Assign5.ipynb            # Main implementation
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ .gitignore
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv                  # Kaggle training data
â”‚   â”œâ”€â”€ test.csv                   # Kaggle test data
â”‚   â””â”€â”€ titanic_survival_predictions_bruce.csv  # Submission file
â””â”€â”€ results/
    â”œâ”€â”€ feature_importance.png
    â”œâ”€â”€ survival_analysis.png
    â””â”€â”€ model_comparison.png
```

---

## ğŸš€ Quick Start

### **1. Install Dependencies**
```bash
pip install pandas numpy scikit-learn xgboost lightgbm matplotlib seaborn
```

### **2. Run the Notebook**
```bash
jupyter notebook Bruce_Assign5.ipynb
```

### **3. Make Predictions**
```python
# Load trained model
final_model = LGBMClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    scale_pos_weight=1.61,
    random_state=42
)
final_model.fit(X_train, y_train)

# Predict on test set
predictions = final_model.predict(X_test)

# Save submission
submission = pd.DataFrame({
    'PassengerId': test_ids,
    'Survived': predictions
})
submission.to_csv('titanic_predictions.csv', index=False)
```

---

## ğŸ“Š Visualizations

The notebook includes:

1. **Missing Value Analysis** - Heatmap of missing data patterns
2. **Survival Rates by Feature** - Bar charts for Sex, Pclass, Title, FamilySize
3. **Feature Distributions** - Age, Fare histograms colored by survival
4. **Correlation Matrix** - Feature relationships and multicollinearity check
5. **Model Comparison** - Bar chart of CV accuracy across 4 models
6. **Feature Importance** - LightGBM feature weights

---

## ğŸ” Model Comparison Details

### **Logistic Regression** (Baseline)
- Validation: 81.01%
- CV: 79.46% Â± 0.016
- **Pros**: Fast, interpretable
- **Cons**: Linear assumptions limit performance

### **Random Forest**
- Validation: 79.33%
- CV: 83.39% Â± 0.036
- **Pros**: Handles non-linearity, feature importance
- **Cons**: Can overfit, slower than gradient boosting

### **XGBoost**
- Validation: 81.01%
- CV: 83.61% Â± 0.022
- **Pros**: State-of-the-art performance, regularization built-in
- **Cons**: More hyperparameters to tune

### **LightGBM** â­
- Validation: 81.01%
- CV: **83.62% Â± 0.033**
- **Pros**: Fastest training, best performance, handles imbalance
- **Cons**: Can overfit on small datasets

**Winner**: LightGBM for best CV accuracy and fastest training.

---

## ğŸ’¡ Why 74.64% on Kaggle (vs 83% CV)?

**Possible Reasons**:
1. **Overfitting to train distribution** - Some patterns don't generalize
2. **Cabin feature noise** - HasCabin may have captured training-specific patterns
3. **Age imputation** - Mean imputation adds noise
4. **Random variation** - Test set may have different characteristics

**Improvement Strategies**:
- More sophisticated age imputation (median by title)
- Feature selection (remove low-importance features)
- Ensemble multiple models
- Cross-validation on different folds

---

## ğŸ”— Related Projects

- **[fraud-detection-imbalanced-learning](../fraud-detection-imbalanced-learning/)** - Advanced class imbalance handling
- **[ensemble-methods-bagging](../ensemble-methods-bagging/)** - Custom ensemble implementation
- **[network-intrusion-detection](../network-intrusion-detection/)** - Feature engineering for cybersecurity
- **[pytorch-cnn-image-classification](../pytorch-cnn-image-classification/)** - Deep learning feature extraction

---

## ğŸ“š References

1. **Dataset**: Kaggle Titanic - Machine Learning from Disaster
2. **Gradient Boosting**: Chen & Guestrin (2016) - "XGBoost: A Scalable Tree Boosting System"
3. **LightGBM**: Ke et al. (2017) - "LightGBM: A Highly Efficient Gradient Boosting Decision Tree"
4. **Feature Engineering**: Zheng & Casari (2018) - "Feature Engineering for Machine Learning"

---

## ğŸ“§ Contact

**Patrick Bruce**
Applied Machine Learning Portfolio
[GitHub](https://github.com/bruce2tech) | [LinkedIn](https://linkedin.com/in/patrick-bruce-97221b17b)

---

## ğŸ“œ License

This project is released under the MIT License for educational and portfolio purposes.

---

**Last Updated**: January 2026
**Status**: âœ… Complete - 74.64% Kaggle Accuracy

---

*"Feature engineering: where domain knowledge meets data science creativity."*
