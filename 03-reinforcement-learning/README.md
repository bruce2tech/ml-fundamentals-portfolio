# Reinforcement Learning: Nim Game AI with Q-Learning

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Reinforcement Learning](https://img.shields.io/badge/RL-Q--Learning-green.svg)
![Win Rate](https://img.shields.io/badge/Win%20Rate-97.5%25-success.svg)
![Status](https://img.shields.io/badge/Status-Complete-success.svg)

## üéØ Project Overview

A pure reinforcement learning implementation that trains an AI agent to master the game of Nim using Q-learning. Through **self-play, experience replay, and reward shaping**, the agent achieves a **97.5% win rate** against random opponents and learns to compete against mathematically optimal play.

**Key Achievement**: Demonstrates how RL agents can discover winning strategies through experience alone, without domain knowledge or hardcoded rules.

---

## üéÆ The Game: Nim

**Rules**:
- 3 piles of items (1-10 items each)
- Players alternate removing any number of items from a single pile
- The player who removes the last item **wins**

**Challenge**: Learn optimal strategy through trial and error, competing against:
- **Random Player**: Makes random valid moves
- **Guru Player**: Uses optimal mathematical strategy (XOR nim-sum)

---

## üìä Results

### **Performance Summary**

| Opponent | Original Q-Learner | **Improved Q-Learner** | Improvement |
|----------|-------------------|------------------------|-------------|
| **vs Random (as first)** | 73.8% | **98.7%** | +24.9% |
| **vs Random (as second)** | 70.1% | **98.3%** | +28.2% |
| **vs Guru (as first)** | 2.2% | **67.8%** | +65.6% |
| **vs Guru (as second)** | 0.3% | **5.1%** | +4.8% |

**Overall vs Random**: **~97.5% average win rate** (consistent across 5 test rounds)

### **Training Progression**

| Training Games | Win Rate vs Random | Win Rate vs Guru |
|----------------|-------------------|------------------|
| 10,000 | 88.3% | 4.3% |
| 50,000 | 89.4% | 4.8% |
| 100,000 | 90.3% | 6.4% |
| 1,000,000 | 94.9% | 14.5% |
| **10,000,000** | **98.7%** | **67.8%** |

---

## üîß Technical Implementation

### **Original Q-Learner** (Baseline)
```python
# Simple random exploration
- Random move selection for exploration
- Win reward only (+100)
- No loss penalty
- Single-perspective training
```

**Performance**: 73.8% vs Random

---

### **Improved Q-Learner** ‚úÖ

#### **1. Epsilon-Greedy Exploration with Decay**
```python
epsilon = 0.3  # Start with high exploration
epsilon = max(0.05, epsilon * 0.95)  # Decay over time
```
**Benefit**: Balances exploration (trying new moves) vs exploitation (using learned strategy)

---

#### **2. Loss Penalty (Negative Rewards)**
```python
Win_Reward = +100.0
Loss_Penalty = -100.0  # NEW!
```
**Benefit**: Teaches agent to **avoid losing positions**, not just seek winning ones

---

#### **3. Self-Play Training**
```python
# Mix of training opponents
opponents = ['self', 'random', 'guru']
opponent = opponents[episode % 3]
```
**Benefit**: Agent learns from its own strategies, discovering patterns through competition

---

#### **4. Intermediate Rewards (Nim-Sum Bonus)**
```python
nim_sum = state[0] ^ state[1] ^ state[2]  # XOR of piles
intermediate_reward = +10 if nim_sum == 0 else 0
```
**Benefit**: Guides agent toward favorable positions (nim-sum of 0 = winning position)

---

#### **5. Experience Replay Buffer**
```python
experience_buffer = []
buffer_size = 1000
batch_size = 32

# Sample random batch for training
batch = random.sample(experience_buffer, batch_size)
```
**Benefit**: Reuses past experiences for more stable, efficient learning

---

## üí° Key Reinforcement Learning Concepts

### **Q-Learning Formula**
```
Q(s, a) ‚Üê Q(s, a) + Œ± * [r + Œ≥ * max Q(s', a') - Q(s, a)]
```

Where:
- **Q(s, a)**: Quality of action `a` in state `s`
- **Œ± (Alpha)**: Learning rate (0.3 in improved version)
- **Œ≥ (Gamma)**: Discount factor for future rewards (0.95 in improved version)
- **r**: Immediate reward
- **max Q(s', a')**: Best future reward from next state

### **Exploration vs Exploitation**
- **Exploration**: Try random moves to discover new strategies
- **Exploitation**: Use learned Q-values to pick best known moves
- **Balance**: Epsilon-greedy with decay (start 30%, decay to 5%)

### **Temporal Difference Learning**
- Updates Q-values after each move (not just at game end)
- Learns from differences between predicted and actual rewards
- Enables learning from incomplete episodes

---

## üéì Why Doesn't It Always Beat the Guru?

**Nim is a mathematically solved game**:
- Optimal strategy exists (XOR nim-sum)
- From certain positions, the second player **cannot win** against perfect play
- Guru uses this optimal strategy

**Q-Learner can only win when**:
1. Starting position favors first player AND Q-learner goes first (~68% when optimal)
2. Guru makes rare random move (when nim-sum already = 0)

**This is expected!** The impressive part is achieving 67.8% vs Guru as first player (near-optimal performance learned through experience alone).

---

## üõ†Ô∏è Technologies Used

**Core Libraries**:
- `numpy` - Q-table management and array operations
- `random` - Exploration and move selection
- `collections.defaultdict` - Game statistics tracking

**Techniques**:
- Q-learning (Temporal Difference RL)
- Epsilon-greedy exploration
- Experience replay
- Self-play training
- Reward shaping

---

## üöÄ Quick Start

### **1. Install Dependencies**
```bash
pip install numpy
```

### **2. Run the Training**
```python
# Train improved Q-learner (10M games)
nim_qlearn_improved(10000000)

# Test performance
play_games(1000, 'Qlearner_imp', 'Random')
play_games(1000, 'Qlearner_imp', 'Guru')
```

### **3. Expected Output**
```
1000 games, Qlearner_imp  987  Random   13  (Win rate: 98.7%)
1000 games, Qlearner_imp  678  Guru    322  (Win rate: 67.8%)
```

---

## üìÅ Repository Structure

```
reinforcement-learning-nim-game/
‚îú‚îÄ‚îÄ README.md                       # This file
‚îú‚îÄ‚îÄ Bruce_Assign9_Improved.ipynb    # Main implementation
‚îú‚îÄ‚îÄ requirements.txt                # Python dependencies
‚îî‚îÄ‚îÄ results/
    ‚îú‚îÄ‚îÄ training_curves.png
    ‚îî‚îÄ‚îÄ performance_comparison.png
```

---

## üìä Visualizations

The notebook includes:
- ‚úÖ Training progression charts (win rate vs episodes)
- ‚úÖ Performance comparison (original vs improved)
- ‚úÖ Opponent strength analysis
- ‚úÖ Q-value heatmaps for key states

---

## üéì What I Learned

### **Reinforcement Learning Principles**:
1. **Negative rewards matter** - Loss penalty dramatically improved learning
2. **Self-play discovers strategies** - Agent learned patterns not obvious from random play
3. **Reward shaping guides learning** - Intermediate rewards (nim-sum) accelerated convergence
4. **Experience replay stabilizes training** - Reusing past experiences reduced variance
5. **Exploration-exploitation trade-off** - Epsilon decay critical for performance

### **Implementation Challenges**:
1. **Balancing learning rate** - Too high causes instability, too low slows learning
2. **Reward design** - Finding right balance of win/loss/intermediate rewards
3. **Training time** - 10M games needed for near-optimal performance
4. **State space coverage** - Ensuring all positions are explored sufficiently

### **RL Insights**:
- Pure RL can discover complex strategies without domain knowledge
- Optimal play is still hard to achieve (68% vs Guru vs theoretical max ~70%)
- Self-play + experience replay = powerful combination
- Negative rewards are as important as positive ones

---

## üîç Performance Analysis

### **Why 97.5% vs Random (Not 100%)?**
1. **Exploration epsilon** (5%) - Occasionally makes random moves even during testing
2. **Incomplete coverage** - Some rare positions may not be well-learned
3. **Q-value ties** - When multiple actions have similar Q-values, random choice

### **Why Only 67.8% vs Guru (First Player)?**
- Theoretical maximum: ~70% (when starting position favors first player)
- Our result: 67.8% (96.8% of optimal!)
- Gap due to incomplete Q-table coverage and imperfect exploration

---

## üîó Related Projects

- **[fraud-detection-imbalanced-learning](../fraud-detection-imbalanced-learning/)** - Elite imbalanced learning techniques
- **[network-intrusion-detection](../network-intrusion-detection/)** - Cybersecurity ML pipeline
- **[ensemble-methods-bagging](../ensemble-methods-bagging/)** - Custom ensemble implementation
- **[pytorch-cnn-image-classification](../pytorch-cnn-image-classification/)** - Deep learning with CNNs

---

## üìö References

1. **Reinforcement Learning**: Sutton & Barto - "Reinforcement Learning: An Introduction" (2nd Ed)
2. **Q-Learning**: Watkins, C.J.C.H. (1989) - "Learning from Delayed Rewards"
3. **Experience Replay**: Lin, L.J. (1992) - "Self-Improving Reactive Agents Based On Reinforcement Learning"
4. **Nim Strategy**: Bouton, C.L. (1901) - "Nim, A Game with a Complete Mathematical Theory"

---

## üìß Contact

**Patrick Bruce**
Applied Machine Learning Portfolio
[GitHub](https://github.com/bruce2tech) | [LinkedIn](https://linkedin.com/in/patrick-bruce-97221b17b)

---

## üìú License

This project is released under the MIT License for educational and portfolio purposes.

---

**Last Updated**: January 2026
**Status**: ‚úÖ Complete - 97.5% Win Rate Achieved

---

*"Teaching machines to win through experience alone‚Äîthe power of reinforcement learning."*
