# Image Classification with PyTorch: From Fully Connected to CNNs

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)
![Computer Vision](https://img.shields.io/badge/Domain-Computer%20Vision-purple.svg)
![Status](https://img.shields.io/badge/Status-Complete-success.svg)

## ğŸ¯ Project Overview

This project demonstrates the **evolution of deep learning approaches** for image classification, comparing traditional fully connected networks with modern Convolutional Neural Networks (CNNs). By working with the Intel Image Classification dataset, I show:

1. **Why fully connected networks fail** on image data (Module 13)
2. **How CNNs solve these problems** through spatial feature learning (Module 14)
3. **The impact of regularization techniques** on model robustness

**Key Result**: Improved test accuracy from **~54% â†’ ~85%** by switching from fully connected to CNN architecture with BatchNorm.

---

## ğŸ“Š Dataset

**Source**: [Intel Image Classification Dataset (Kaggle)](https://www.kaggle.com/puneet6060/intel-image-classification)

**Characteristics**:
- **6 landscape categories**: Buildings, Forest, Glacier, Mountain, Sea, Street
- **Training set**: 14,034 images (128Ã—128 RGB)
- **Test set**: 3,000 images (128Ã—128 RGB)
- **Class distribution**: Relatively balanced (~2,000-2,500 per class)
- **Challenges**: Intra-class variation, inter-class similarity (mountain vs glacier)

**Sample Images**:
```
Buildings  Forest  Glacier  Mountain  Sea  Street
   ğŸ™ï¸       ğŸŒ²       â›°ï¸        ğŸ”ï¸      ğŸŒŠ    ğŸš—
```

---

## ğŸ”¬ Approach: A Learning Journey

### **Phase 1: Fully Connected Neural Network (Module 13)** âŒ

#### **Architecture**:
```
Input: 128Ã—128Ã—3 = 49,152 features (flattened)
  â†“
Hidden Layer 1: 50 nodes + ReLU
  â†“
Hidden Layer 2: 50 nodes + ReLU
  â†“
Hidden Layer 3: 50 nodes + ReLU
  â†“
Output: 6 classes (softmax)
```

**Total Parameters**: 2.46 million (mostly in first layer!)

#### **Results**:
| Metric | Without Regularization | With Weight Decay |
|--------|----------------------|-------------------|
| **Train Accuracy** | ~85-95% | ~65-68% |
| **Test Accuracy** | ~54-58% | ~55-57% |
| **Verdict** | ğŸ”´ Severe Overfitting | ğŸŸ¡ Poor Generalization |

#### **Why It Failed**:

**1. Spatial Structure Destroyed**
- Flattening 128Ã—128Ã—3 â†’ 49,152 vector loses 2D relationships
- A tree in top-left vs bottom-right treated as completely different
- No concept of "nearby pixels" or local patterns

**2. No Translation Invariance**
- Same object in different positions = different input patterns
- Model must learn each position separately
- Wastes parameters on redundant learning

**3. Parameter Explosion**
- First layer: 49,152 â†’ 50 = **2.45M parameters**
- Can't learn meaningful features with limited data
- Prone to overfitting despite regularization

**4. No Feature Hierarchy**
- Can't build edges â†’ shapes â†’ objects progression
- Each layer sees raw, unstructured features
- Misses compositionality of visual concepts

---

### **Phase 2: Convolutional Neural Networks (Module 14)** âœ…

#### **Architecture**:
```
Input: 128Ã—128Ã—3 RGB image
  â†“
Conv2d(3â†’32, 5Ã—5) + BatchNorm + ReLU + MaxPool(2Ã—2)  # 64Ã—64Ã—32
  â†“
Conv2d(32â†’64, 5Ã—5) + BatchNorm + ReLU + MaxPool(2Ã—2)  # 32Ã—32Ã—64
  â†“
Conv2d(64â†’128, 3Ã—3) + BatchNorm + ReLU + MaxPool(2Ã—2) # 16Ã—16Ã—128
  â†“
AdaptiveAvgPool(4Ã—4)  # 4Ã—4Ã—128
  â†“
Flatten â†’ FC(2048â†’128) + BatchNorm + ReLU + Dropout(0.2)
  â†“
FC(128â†’6) + Softmax
```

**Total Parameters**: ~2.5M (distributed efficiently across layers)

#### **Results**:

| Configuration | Train Acc | Test Acc | Notes |
|---------------|-----------|----------|-------|
| **Baseline CNN** | ~87% | ~82% | Good performance |
| **+ BatchNorm + Dropout** | ~95% | ~85% | Best generalization âœ“ |

**Note**: CNN baseline already achieves strong performance (~82%) due to proper architecture. BatchNorm improves convergence speed and adds ~3% test accuracy.

#### **Why It Succeeds**:

**1. Preserves Spatial Structure**
- Convolution operates on 2D neighborhoods
- Learns local patterns (edges, textures, shapes)
- Hierarchical feature building: edges â†’ parts â†’ objects

**2. Translation Invariance**
- Same filter applied across entire image
- Object detected regardless of position
- Parameter sharing drastically reduces model size

**3. Efficient Parameter Usage**
- First conv layer: 3Ã—32Ã—5Ã—5 = **2,400 parameters** (vs 2.45M!)
- Shared filters learn generalizable patterns
- More data-efficient learning

**4. Feature Hierarchy**
- **Layer 1** (32 filters): Edges, colors, simple textures
- **Layer 2** (64 filters): Shapes, patterns, corners
- **Layer 3** (128 filters): Complex objects, semantic parts
- **FC Layers**: Combine features for classification

---

## ğŸ“ˆ Performance Comparison

### **Accuracy Improvement**

```
Fully Connected:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  ~54-58%
CNN (Baseline):   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘  ~82%
CNN + BatchNorm:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘  ~85%

Improvement: ~30 percentage points over FC (+55% relative improvement)
```

### **Confusion Matrix Analysis**

**Fully Connected** (Major Confusions):
- Buildings â†” Street (urban scenes)
- Mountain â†” Glacier (snowy peaks)
- Sea â†” Glacier (blue colors)

**CNN with BatchNorm** (Much Better):
- Clear class separation
- Fewer cross-category errors
- Better handling of similar scenes

---

## ğŸ”§ Key Techniques Implemented

### **1. Batch Normalization**
**Purpose**: Stabilize training by normalizing layer inputs
**Impact**:
- Faster convergence
- Higher learning rates possible
- Acts as regularization
- **Result**: ~3% test accuracy improvement over CNN baseline

### **2. Dropout (p=0.2)**
**Purpose**: Prevent co-adaptation of neurons
**Impact**:
- Reduces overfitting
- More robust predictions
- Better generalization

### **3. Early Stopping (patience=3)**
**Purpose**: Stop training before overfitting
**Impact**:
- Prevents memorization
- Saves training time
- Preserves best validation performance

### **4. Weight Decay (5e-5)**
**Purpose**: L2 regularization on weights
**Impact**:
- Prevents large weights
- Smoother decision boundaries
- Better generalization

---

## ğŸ’¡ Lessons Learned

### **Architectural Insights**:

1. **CNNs are essential for images**
   - Fully connected networks waste parameters
   - Spatial structure matters
   - Translation invariance is critical

2. **Regularization prevents overfitting**
   - Batch normalization: biggest single improvement
   - Dropout: adds robustness
   - Early stopping: prevents memorization

3. **Feature hierarchy is powerful**
   - Low-level features (edges) reused across classes
   - High-level features (semantic parts) are class-specific
   - Compositionality matches human visual perception

### **Training Best Practices**:

1. **Monitor train vs test gap**
   - Large gap = overfitting
   - Use regularization aggressively

2. **Batch normalization placement**
   - After convolution, before activation
   - Also in fully connected layers

3. **Learning rate matters**
   - Too high: unstable training
   - Too low: slow convergence
   - Adam optimizer handles this well

4. **Data normalization is crucial**
   - [0, 1] scaling for images
   - Consistent preprocessing for train/test

---

## ğŸ› ï¸ Technologies & Tools

**Core Framework**:
- `PyTorch` - Deep learning framework
- `torchvision` - Image datasets and transforms

**Data Processing**:
- `OpenCV (cv2)` - Image loading and preprocessing
- `NumPy` - Array operations
- `pandas` - Data organization

**Visualization**:
- `matplotlib` - Plotting
- `seaborn` - Statistical visualizations

**Evaluation**:
- `scikit-learn` - Metrics and evaluation tools

---

## ğŸ“ Project Structure

```
05-deep-learning-progression/
â”œâ”€â”€ README.md                              # This file
â”œâ”€â”€ deep_learning_image_classification.ipynb  # Combined FC vs CNN implementation
â”œâ”€â”€ requirements.txt                       # Python dependencies
â”œâ”€â”€ .gitignore                             # Excludes dataset from git
â”œâ”€â”€ archive/                               # Intel Image dataset (download from Kaggle)
â”‚   â”œâ”€â”€ seg_train/
â”‚   â””â”€â”€ seg_test/
â””â”€â”€ results/                               # Visualization outputs
    â”œâ”€â”€ model_performance_fc_vs_cnn.png
    â””â”€â”€ training_progression_fc_vs_cnn.png
```

---

## ğŸš€ Quick Start

### **1. Download Dataset**
```bash
# Download from Kaggle:
# https://www.kaggle.com/puneet6060/intel-image-classification
# Extract to archive/ folder within the project directory
```

### **2. Install Dependencies**
```bash
pip install torch torchvision numpy matplotlib seaborn scikit-learn
```

### **3. Run the Notebook**
```bash
jupyter notebook deep_learning_image_classification.ipynb
```

The notebook compares all approaches (FC, FC+WeightDecay, CNN, CNN+BatchNorm) in a single file.

---

## ğŸ“Š Visualizations Included

### **Phase 1 (Fully Connected)**:
- âœ… Sample images from each class
- âœ… BGR vs RGB color conversion demo
- âœ… Training/test accuracy curves (showing overfitting)
- âœ… Confusion matrix (showing poor performance)

### **Phase 2 (CNN)**:
- âœ… Architecture diagram
- âœ… Training curves with regularization comparison
- âœ… Feature map visualizations
- âœ… Confusion matrices (before/after improvements)
- âœ… Per-class accuracy breakdown

---

## ğŸ“ Key Takeaways

### **For Hiring Managers**:

This project demonstrates:
1. **Problem diagnosis**: Identified why fully connected fails
2. **Solution design**: Applied CNNs with proper regularization
3. **Iterative improvement**: Baseline â†’ +regularization â†’ +BatchNorm
4. **Production awareness**: Considered overfitting, generalization, robustness
5. **Clear communication**: Explained technical decisions in business terms

### **Technical Growth**:
- Deep understanding of CNN advantages over fully connected
- Hands-on experience with regularization techniques
- Ability to diagnose and fix overfitting
- PyTorch proficiency (custom training loops, layer design)

---

## ğŸ”— Related Projects

- **Credit Card Fraud Detection** (Module 12): Imbalanced learning with PyTorch
- **Reinforcement Learning** (Module 9): Q-learning with neural networks
- **Ensemble Methods** (Module 6): Bagging and model comparison

---

## ğŸ“§ Contact

**Patrick Bruce**
Applied Machine Learning Portfolio

---

## ğŸ“š References

1. **Dataset**: Intel Image Classification (Kaggle)
2. **Architecture**: Raschka et al., "Machine Learning with PyTorch and Scikit-Learn" (Chapter 14)
3. **Batch Normalization**: Ioffe & Szegedy (2015) - "Batch Normalization: Accelerating Deep Network Training"
4. **CNNs**: LeCun et al. (1998) - "Gradient-Based Learning Applied to Document Recognition"
5. **PyTorch Docs**: [CNN Tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)

---

**Last Updated**: January 2026
**Status**: âœ… Complete - Shows Learning Progression
